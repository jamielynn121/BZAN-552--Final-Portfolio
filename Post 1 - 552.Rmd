---
title: "Post 1 - Board Games"
author: "Jamie McKinnon"
date: "December 1, 2018"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.path = "README_figs/README-"
)
```

```{r Load Packages}
library(DataExplorer)
library(tidyverse)
library(cluster)
library(factoextra)
library(dendextend)
library(dplyr)
```

# Introduction
The board game industry is an 9.6 billion industry, globally as of 2016, and constantly growing. Game analysis is sought by both the publishers and consumers. BoardGameGeek.com allows users to add their colletion information, including price paid, their ranking, and number of plays of each game owned. 

With sites like BGG.com making it available for hobbiest to publish their collections, data is readily available for publishers to evaluate groupings and clusters of games to potentially influence where to focus marketing for new games and kickstarter funding campaigns. 

# Data Pull
All collection information is manually entered over time and is unique to an individuals account. Some accounts are made public, so their collection is viewable by others. To download a .csv of my personal collection, go to https://boardgamegeek.com/collection/user/Astrodar?own=1&subtype=boardgame&ff=1 and click "owned" next to Download board games: above the table of games. This will initialize a download to your computer.

# Data Read
First we read the data into R, and look at which columns are majority missing values using the DataExplorer package. We remove those columns from our collection data.

```{r Read Data}
#collection <- read.csv("C:/Users/jamie/Downloads/BZAN552_FinalPortfolio/collection.csv")
collection <- read.csv("C:/Users/jamie/Documents/BZAN/BZAN-552--Final-Portfolio/collection.csv")
head(collection)

DataExplorer::plot_missing(collection)

remove_cols <- c("quantity", "version_yearpublished", "invdate", "invlocation", "cv_currency", "currvalue", "barcode", "other", "language", "year", "imageid" , "publisherid", "wantpartslist", "haspartslist", "conditiontext", "comment")

collection <- collection[ , !(names(collection) %in% remove_cols)]
```

# Data Insight
Because this data is based on my personal game collection, the set isn't very large. I own 167 unique games and expansions. However, because I track number of plays to determine how many games I've played in a year, I don't track expansion plays when they are used in conjunction with the base game. 

To perform cluster analysis, I am intersted in the continuous variables only that I think would play a roll in clustering board games. 

Unfortunately, a lot of my collection were gifts and I dont have the price paid for half of my collection. I am interested in clustering based on price and number of plays but in conjunction, there are few games that overlap to produce good clusters.

# Data Subsets
I separate into two data subsets and I will perform separate cluster analysis on the two data subsets - (1) with price paid and without number of plays (2) with number of plays and without price plid. 

```{r Data Subsets}
# Continuous variables only
continuous <- c("rating", "numplays", "baverage", "minplayers", "maxplayers", "playingtime", "pricepaid")
col_cont <- collection[continuous]

# DATA SUBSETS

##------------##
## Price Paid ##
##------------##
# Where we know the price paid
summary(collection$pricepaid)
known_price <- collection[which(!is.na(collection$pricepaid)),]
pri_cont <- known_price[continuous]

## How many na values do we have in this data
sum(sapply(pri_cont, is.na))
sum(sapply(pri_cont, is.infinite))
sum(sapply(pri_cont, is.nan))

# There are too many NA values in num plays for the data set where we know the price paid - so we can remove that column
pri_cont <- pri_cont[,-which(names(pri_cont) == "numplays")]
sum(sapply(pri_cont, is.na))

##--------------##
## Number Plays ##
##--------------##
# Games we've played
# Expansions to games are never marked as "played" unless they can be played as a stand-alone game
num_plays <- collection[which(!is.na(collection$numplays)),]
play_cont <- num_plays[continuous]

## How many na values do we have in this data
sum(sapply(play_cont, is.na))
sum(sapply(play_cont, is.infinite))
sum(sapply(play_cont, is.nan))

# There are too many NA values in num plays for the data set where we know the price paid - so we can remove that column
play_cont <- play_cont[,-which(names(play_cont) == "pricepaid")]
sum(sapply(play_cont, is.na))
```

# Hierarchical Agglomerative Clustering with Price Data Subset
First, I perform agglomerative (bottom up) clustering on my price data subset. I noticed after my first run that if a personal rating for that game did not exist then it was marked zero, rather than NA. So I reduce the data subset to not include unrated games.

I ensure there are no NA, infiniate, or NaN values in the data set and only use complete cases in my analysis. This reduced set is 61 observations, compared to 167 observations from the full collection. The data is scaled to standardize all variables. This ensures one column is not weighted much higher than another column due to original scale of the data and change in values.

There are multiple methods that can be used in hierarchical clustering. Here, I assess the average, single ,complete, and ward methods to determine which method has the highest agglomerative coefficient (AC). The AC measures the strength of the clustering structure. The closer the cluster structure is to 1, the better. Here, the ward method provides the highest AC so this is the method I choose to proceed with in my clustering. 

To determine the optimal number of clusters, I use the silhouette calculation that produces a plot of the number of clusters versus the average sihoulette width. The average sihouette method measure the quality of clustering, like the agglomerative coefficient used above. Used together, we can determine how well each object lies within its cluster. A high average sihouette indicates good clustering. Three is determined to be the optimal number of clusters, as it produces the highest average sihouette width.  

Finally, building my fit, I use the euclidean distance as the metric for measuring distance between elements to be merged and the ward.D2 method to lead to the minimum increase in total within-cluster variance after merging. Usually, the closest elements by distance that minimize the total within-cluster variance are merged to create a new cluster. This step is repeated for all points until they all join to one at the very top of the dendogram. 

```{r Hierarchical Price}
# BGG changes ratings to 0 if they haven't been rated
# Must change these to NAs
pri_cont <- pri_cont[which(!pri_cont$rating == "0"),]


# How many na values do we have in this data
sum(sapply(pri_cont, is.na))
sum(sapply(pri_cont, is.infinite))
sum(sapply(pri_cont, is.nan))

# Keep only complete classes
pri_cont_complete <- pri_cont[complete.cases(pri_cont),]

# Standardize all variables
pri_cont_scale  <- scale(pri_cont_complete)

# Which method is most appropriate for clustering? 
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# Function to compute agglomerative coefficient - measures the amount of clustering structure found
ac <- function(x) {
  agnes(pri_cont_scale, method = x, metric = "euclidean")$ac
}

map_dbl(m, ac)

# Determine Optimal Number of Clusters - Three is determined to be best by the silhouette calculation
fviz_nbclust(pri_cont_scale, hcut, method = "silhouette", hc_method = "ward.D2")

d <- dist(pri_cont_scale, method = "euclidian") # distance matrix
fit <- hclust(d, method="ward.D2")

## Display Dendogram ##
avg_dend_obj <- as.dendrogram(fit)
avg_col_dend <- color_branches(avg_dend_obj, h = 10)
plot(avg_col_dend)

```

# Three Clusters on Price Data Subset
I cut the into three clusters, as determined optimal, and add that cluster ID back into the original data for further analysis. First, I summarize the clusters by mean values of each continuous variable to see how the clusters are different. 

The three clusters have very different average price points and playing times. The max players and min players are, however, very similar. Cluster 1 seems to have higher ratings, higher playing times and higher price paid. While cluster 2 seems to be middle ground between 1 and 3. Cluster three is made up of 1 game that is an outlier in the data with a very low rating and very low price that plays very quickly. 

Then, we can look at the game names and other statistics that were not used in the cluster analysis. Upon looking at the name of the games in each respective cluster, it is obvious to me why they were grouped like this. Cluster 1 includes some of my favorite games, games I was willing to pay a lot more for and that take a longer time to play. The ratings are highest because they are the games I have enjoyed the most in my collection. Cluster 2 includes a lot of games I really enjoy, but they play a little quicker and I haven't paid nearly as much for these games. Cluster 3 is a game, Zombie Dice, I got from a sale on International Table Top day. The sale was really good and my price paid was really low. Overall, the game was okay, but it isn't one that hits the table a lot anymore.

```{r Three Clusters on Price Subset}
# cluster ID for each observation when cutting tree into 3 clusters
k3 <- cutree(fit, k=3) 

k3 <- as.factor(k3)
length(k3)

nrow(pri_cont_complete)
# Add the cluster number as a column in my data so we can group by
pri_cont_complete$k3 <- k3

pri_cont_complete %>%
  group_by(k3) %>%
  summarise_all("mean")

# Add the cluster number back into the original dataset for easy distinction
known_price_complete <- known_price[which(!known_price$rating == "0"),]
nrow(known_price_complete)
known_price_complete$k3 <- k3

# Games in cluster 1
known_price_complete$objectname[which(known_price_complete$k3 == 1)]
table(known_price_complete$itemtype[which(known_price_complete$k3 == 1)])
table(droplevels(known_price_complete$acquiredfrom[which(known_price_complete$k3 == 1)]))
known_price_complete[which(known_price_complete$k3 == 1),]

# Games in cluster 2
known_price_complete$objectname[which(known_price_complete$k3 == 2)]
table(known_price_complete$itemtype[which(known_price_complete$k3 == 2)])
table(droplevels(known_price_complete$acquiredfrom[which(known_price_complete$k3 == 2)]))
known_price_complete[which(known_price_complete$k3 == 2),]

# Games in cluster 3
known_price_complete$objectname[which(known_price_complete$k3 == 3)]
known_price_complete$itemtype[which(known_price_complete$k3 == 3)]
known_price_complete$acquiredfrom[which(known_price_complete$k3 == 3)]
known_price_complete[which(known_price_complete$k3 == 3),]
```

# K-means Clustering on Price Data Subset
K-means clustering was my second choice for clustering, but I wanted to see how the clusters fit in reduced dimensions to see if this method was appropriate. 

K-means clustering is found to work well when the shape of the clusters is hyper-spherical like circle in 2D. K-means clustering requries you know the number of clusters to intialize the fit. Furthermore, it does not do well when there are outliers in the cluster. 

Wards method for hierarchical clustering is the hierarchical analog of K-means and can be used to initalize a k-means cluster. So here, I initilaize the kmeans fit with three clusters. 

By plotting the clusters after kmeans, we see that Zombie Dice (game 167) is still an outlier in this data set, but not included in a cluster by itself like it was above in cluster 3 in the hierarchical method. Instead, it skews cluster 3 from the kmeans method and is very different from the centroid of that cluster.

```{r Kmeans Price}
set.seed(552)

fit <- kmeans(pri_cont_scale, 3)
fit         # print all available components
fit$center  # centers of each variable
fit$cluster # cluster ID for each observation

fviz_cluster(fit, data = pri_cont_scale, palette = "Set2", ggtheme = theme_minimal())

# Determine number of clusters with SSE
SSEs <- rep(NA,10) # a vector to store SSEs for different k's
SSEs[1] <- fit$totss # total SSE if no clustering is done
for(k in 2:10){
	fit <- kmeans(pri_cont_scale,k)
	SSEs[k] <- fit$tot.withinss
}
par(mar=c(4,4,1,1))

plot(1:10,SSEs,type="b",xlab="Number of Clusters")

```

# Hierarchical Agglomerative Clustering with Number of Plays
All techniques for clustering were duplicated from the hierarchical analysis above. Therefore, I will only explain the differences I see in the output of the clusters. 

```{r Hierarchical Number of Plays}
# BGG changes ratings to 0 if they haven't been rated
# Must change these to NAs
play_cont <- play_cont[which(!play_cont$rating == "0"),]


# How many na values do we have in this data
sum(sapply(play_cont, is.na))
sum(sapply(play_cont, is.infinite))
sum(sapply(play_cont, is.nan))

# Keep only complete classes
play_cont_complete <- play_cont[complete.cases(play_cont),]

# standardize all variables
play_cont_scale  <- scale(play_cont_complete)

## Which method is most appropriate for clustering? ##
# Methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# Function to compute agglomerative coefficient
# Measures the amount of clustering structure found
ac <- function(x) {
  agnes(play_cont_scale, method = x, metric = "euclidean")$ac
}

map_dbl(m, ac)

## Determine Optimal Number of Clusters ##
fviz_nbclust(play_cont_scale, hcut, method = "silhouette", hc_method = "ward.D2")
# Three is determined to be best by the silhouette calculation

d <- dist(play_cont_scale, method = "euclidian") # distance matrix
fit <- hclust(d, method="ward.D2")

## Display Dendogram ##
avg_dend_obj <- as.dendrogram(fit)
avg_col_dend <- color_branches(avg_dend_obj, h = 13)
plot(avg_col_dend)
```


# Three Clusters on Number of Plays Data Subset
I performed the same type of clustering as the hierarchical method above with the price paid column. Here, cluster 2 represents similar games with some overlap as cluster 1 from the first hierarchical model. These are games with high ranking, high number of plays, and long playing times. These also seem to have a lower average player count. This makes sense because a lot of the games I enjoy playing are with small groups of 2-4 to allow the most interactivity. Cluster 3 is the same as before, low price and low ratings. It includes one more game than before that was left out of the last subset. These are both games that rarely hit the table, and are pretty quick party games for large groups of people. I purchased these games specifically for that purpose of large group play and social events. I prefer heavy board games to light board games which is why these are generally ranked lower. Cluster 1 is that middle group with games I enjoy, but arn't my favorite. 


```{r Three Clusters on Number of Plays Subset}
# cluster ID for each observation when cutting tree into 3 clusters
k3 <- cutree(fit, k=3) 

k3 <- as.factor(k3)
length(k3)

nrow(play_cont_complete)
# Add the cluster number as a column in my data so we can group by
play_cont_complete$k3 <- k3

play_cont_complete %>%
  group_by(k3) %>%
  summarise_all("mean")


num_plays_complete <- num_plays[which(!num_plays$rating == "0"),]
nrow(num_plays_complete)
num_plays_complete$k3 <- k3

# Games in cluster 1
num_plays_complete$objectname[which(num_plays_complete$k3 == 1)]
table(num_plays_complete$itemtype[which(num_plays_complete$k3 == 1)])
table(droplevels(num_plays_complete$acquiredfrom[which(num_plays_complete$k3 == 1)]))

# Games in cluster 2
num_plays_complete$objectname[which(num_plays_complete$k3 == 2)]
num_plays_complete[which(num_plays_complete$k3 == 2),]
table(droplevels(num_plays_complete$acquiredfrom[which(num_plays_complete$k3 == 2)]))

# Games in cluster 3
num_plays_complete$objectname[which(num_plays_complete$k3 == 3)]
num_plays_complete[which(num_plays_complete$k3 == 3),]
table(droplevels(num_plays_complete$acquiredfrom[which(num_plays_complete$k3 == 3)]))

```

# K-means Clustering on Number of Plays Data Subset
Similar to the kmeans clustering above, we use the number of clusters detected from the ward hierarchical model. Three clusters are initialized and plotted on two reduced dimensions. There is a little cluster overlap between cluster 2 and three here. We do not see the small cluster of 1 or 2 games like we have in previous models, but there are outliers in both cluster 1 and 3 that skew the clusters. These points seem to be farther from the centroid of their respective cluster. This model is less repeatable and several of the clusters changed when setting the seed differently. This produces less reliable clusters that, and this method seems like it would be less beneificial to for marketing purposes.

```{r Kmeans Number of Plays}
set.seed(1)

fit <- kmeans(play_cont_scale, 3)
fit         # print all available components
fit$center  # centers of each variable
fit$cluster # cluster ID for each observation

fviz_cluster(fit, data = play_cont_scale, palette = "Set2", ggtheme = theme_minimal())

# Determine number of clusters with SSE
SSEs <- rep(NA,10) # a vector to store SSEs for different k's
SSEs[1] <- fit$totss # total SSE if no clustering is done
for(k in 2:10){
	fit <- kmeans(play_cont_scale,k)
	SSEs[k] <- fit$tot.withinss
}
par(mar=c(4,4,1,1))

plot(1:10,SSEs,type="b",xlab="Number of Clusters")

```


# Conclusion
In conclusion, the hierarchical methods used for both data subsets could be used for marketing and cluster placement of new games. Based on cluster placement, consumers could be targeted who like similar games that fall within the same cluster. Besides pulling the data set from BoardGameGeek of ones own personal collection, you can pull everyones information who publically publishes their collection information. Using this large sample, targeting users who do not own the game yet, but may like the game would be a key technique. 